{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyTorch\n",
    "import torch\n",
    "\n",
    "list_a = [1, 2, 3, 4]\n",
    "\n",
    "# Create a tensor from list_a\n",
    "tensor_a = torch.tensor(list_a)\n",
    "\n",
    "# Display the tensor device\n",
    "print(tensor_a.device)\n",
    "\n",
    "# Display the tensor data type\n",
    "print(tensor_a.dtype)\n",
    "\n",
    "# Create two tensors from the arrays\n",
    "tensor_a = torch.tensor(array_a)\n",
    "tensor_b = torch.tensor(array_b)\n",
    "\n",
    "# Subtract tensor_b from tensor_a \n",
    "tensor_c = tensor_a - tensor_b\n",
    "\n",
    "# Multiply each element of tensor_a with each element of tensor_b\n",
    "tensor_d = tensor_a * tensor_b\n",
    "\n",
    "# Add tensor_c with tensor_d\n",
    "tensor_e = tensor_c + tensor_d\n",
    "print(tensor_e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_tensor = torch.Tensor([[2, 3, 6, 7, 9, 3, 2, 1]])\n",
    "\n",
    "# Implement a small neural network with exactly two linear layers\n",
    "model = nn.Sequential(nn.Linear(8,16),\n",
    "                      nn.Linear(16,1)\n",
    "                     )\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.tensor([[0.8]])\n",
    "\n",
    "# Create a sigmoid function and apply it on input_tensor\n",
    "sigmoid = nn.Sigmoid()\n",
    "probability = sigmoid(input_tensor)\n",
    "print(probability)\n",
    "\n",
    "input_tensor = torch.tensor([[1.0, -6.0, 2.5, -0.3, 1.2, 0.8]])\n",
    "\n",
    "# Create a softmax function and apply it on input_tensor\n",
    "softmax = nn.Softmax(dim=-1)\n",
    "probabilities = softmax(input_tensor)\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# Implement a small neural network for binary classification\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(8,1),\n",
    "  nn.Sigmoid()\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 7, 10, 12, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# Implement a neural network with exactly four linear layers\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(11,33),\n",
    "    nn.Linear(33,22),\n",
    "    nn.Linear(22,5),\n",
    "    nn.Linear(5,1))\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiclass with 4 classes\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 7, 10, 12, 2, 3, 6, 8, 9]])\n",
    "\n",
    "n_classes = 4\n",
    "\n",
    "# Update network below to perform a multi-class classification with four labels\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(11, 20),\n",
    "  nn.Linear(20, 12),\n",
    "  nn.Linear(12, 6),\n",
    "  nn.Linear(6, n_classes), \n",
    "  nn.Softmax(dim=-1)\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 1\n",
    "num_classes = 3\n",
    "\n",
    "# Create the one-hot encoded vector using NumPy\n",
    "one_hot_numpy = np.array([0, 1, 0])\n",
    "\n",
    "# Create the one-hot encoded vector using PyTorch\n",
    "one_hot_pytorch = F.one_hot(torch.tensor(y), num_classes = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "y = [2]\n",
    "scores = torch.tensor([[0.1, 6.0, -2.0, 3.2]])\n",
    "\n",
    "# Create a one-hot encoded vector of the label y\n",
    "one_hot_label = F.one_hot(torch.tensor(y), scores.shape[1])\n",
    "\n",
    "# Create the cross entropy loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Calculate the cross entropy loss\n",
    "loss = criterion(scores.double(), one_hot_label.double())\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Calculate the loss\n",
    "loss = criterion(preds, target)\n",
    "\n",
    "# Compute the gradients of the loss\n",
    "loss.backward() \n",
    "\n",
    "# Display gradients of the weight and bias tensors in order\n",
    "print(weight.grad)\n",
    "print(bias.grad)\n",
    "\n",
    "model = nn.Sequential(nn.Linear(16, 8),\n",
    "                      nn.Sigmoid(),\n",
    "                      nn.Linear(8, 2))\n",
    "\n",
    "# Access the weight of the first linear layer\n",
    "weight_0 = model[0].weight\n",
    "\n",
    "# Access the bias of the second linear layer\n",
    "bias_1 = model[2].bias\n",
    "\n",
    "weight0 = model[0].weight\n",
    "weight1 = model[1].weight\n",
    "weight2 = model[2].weight\n",
    "\n",
    "# Access the gradients of the weight of each linear layer\n",
    "grads0 = weight0.grad\n",
    "grads1 = weight1.grad\n",
    "grads2 = weight2.grad\n",
    "\n",
    "# Update the weights using the learning rate and the gradients\n",
    "weight0 = weight0 - lr * grads0\n",
    "weight1 = weight1 - lr * grads1\n",
    "weight2 = weight2 - lr * grads2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tourch.optim as optim\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "loss = criterion(pred, target)\n",
    "loss.backward()\n",
    "\n",
    "# Update the model's parameters using the optimizer\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = np.array(10)\n",
    "y = np.array(1)\n",
    "\n",
    "# Calculate the MSELoss using NumPy\n",
    "mse_numpy = np.mean((y_hat - y)**2)\n",
    "\n",
    "# Create the MSELoss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Calculate the MSELoss using the created loss function\n",
    "mse_pytorch = criterion(torch.tensor(y_hat).float(), torch.tensor(y).float())\n",
    "print(mse_pytorch)\n",
    "print(mse_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the number of epochs and the dataloader\n",
    "for i in range(num_epochs):\n",
    "  for data in dataloader:\n",
    "    # Set the gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "    # Run a forward pass\n",
    "    feature, target = data\n",
    "    prediction = model(feature)    \n",
    "    # Calculate the loss\n",
    "    loss = criterion(prediction, target)    \n",
    "    # Compute the gradients\n",
    "    loss.backward()\n",
    "    # Update the model's parameters\n",
    "    optimizer.step()\n",
    "show_results(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ReLU function with PyTorch\n",
    "relu_pytorch = nn.ReLU()\n",
    "\n",
    "# Apply your ReLU function on x, and calculate gradients\n",
    "x = torch.tensor(-1.0, requires_grad=True)\n",
    "y = relu_pytorch(x)\n",
    "y.backward()\n",
    "\n",
    "# Print the gradient of the ReLU function for x\n",
    "gradient = x.grad\n",
    "print(gradient)\n",
    "\n",
    "# Create a leaky relu function in PyTorch\n",
    "leaky_relu_pytorch = nn.LeakyReLU(negative_slope = 0.05)\n",
    "\n",
    "x = torch.tensor(-2.0)\n",
    "# Call the above function on the tensor x\n",
    "output = leaky_relu_pytorch(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 8\n",
    "n_classes = 2\n",
    "\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# Create a neural network with less than 120 parameters\n",
    "model = nn.Sequential(nn.Linear(n_features,3),\n",
    "                        nn.Linear(3,4),\n",
    "                        nn.Linear(4,n_classes))\n",
    "output = model(input_tensor)\n",
    "\n",
    "print(calculate_capacity(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():    \n",
    "  \n",
    "    # Check if the parameters belong to the first layer\n",
    "    if name == '0.weight' or name == '0.bias':\n",
    "      \n",
    "        # Freeze the parameters\n",
    "        param.requires_grad = False\n",
    "  \n",
    "    # Check if the parameters belong to the second layer\n",
    "    if name == '1.weight' or name == '1.bias':\n",
    "      \n",
    "        # Freeze the parameters\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer0 = nn.Linear(16, 32)\n",
    "layer1 = nn.Linear(32, 64)\n",
    "\n",
    "# Use uniform initialization for layer0 and layer1 weights\n",
    "nn.init.uniform_(layer0.weight)\n",
    "nn.init.uniform_(layer1.weight)\n",
    "\n",
    "model = nn.Sequential(layer0, layer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "np_features = np.array(np.random.rand(12, 8))\n",
    "np_target = np.array(np.random.rand(12, 1))\n",
    "\n",
    "# Convert arrays to PyTorch tensors\n",
    "torch_features = torch.tensor(np_features)\n",
    "torch_target = torch.tensor(np_target)\n",
    "\n",
    "# Create a TensorDataset from two tensors\n",
    "dataset = TensorDataset(torch_features, torch_target)\n",
    "\n",
    "# Return the last element of this dataset\n",
    "print(dataset[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When pulling data froma Pandas dataframe\n",
    "# Load the different columns into two PyTorch tensors\n",
    "features = torch.tensor(dataframe[['ph', 'Sulfate', 'Conductivity', 'Organic_carbon']].to_numpy()).float()\n",
    "target = torch.tensor(dataframe['Potability'].to_numpy()).float()\n",
    "\n",
    "# Create a dataset from the two generated tensors\n",
    "dataset = TensorDataset(features, target)\n",
    "\n",
    "# Create a dataloader using the above dataset\n",
    "dataloader = DataLoader(dataset, shuffle=True, batch_size=2)\n",
    "x, y = next(iter(dataloader))\n",
    "\n",
    "# Create a model using the nn.Sequential API\n",
    "model = nn.Sequential(nn.Linear(4,6),\n",
    "                        nn.Linear(6,1))\n",
    "output = model(features)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "validation_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "  \n",
    "  for data in validationloader:\n",
    "    \n",
    "      outputs = model(data[0])\n",
    "      loss = criterion(outputs, data[1])\n",
    "      \n",
    "      # Sum the current loss to the validation_loss variable\n",
    "      validation_loss += loss.item()\n",
    "      \n",
    "# Calculate the mean loss value\n",
    "validation_loss_epoch = validation_loss / len(validationloader)\n",
    "print(validation_loss_epoch)\n",
    "\n",
    "# Set the model back to training mode\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import touchmetrics\n",
    "\n",
    "# Create accuracy metric using torch metrics\n",
    "metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=3)\n",
    "for data in dataloader:\n",
    "    features, labels = data\n",
    "    outputs = model(features)\n",
    "    \n",
    "    # Calculate accuracy over the batch\n",
    "    acc = metric(outputs.softmax(dim=-1), labels.argmax(dim=-1))\n",
    "    \n",
    "# Calculate accuracy over the whole epoch\n",
    "acc = metric.compute()\n",
    "\n",
    "# Reset the metric for the next epoch \n",
    "metric.reset()\n",
    "plot_errors(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small neural network\n",
    "model = nn.Sequential(nn.Linear(3072,16),nn.ReLU(), nn.Dropout(p=0.5))\n",
    "model(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = []\n",
    "for idx in range(10):\n",
    "    # Randomly sample a learning rate factor between 2 and 4\n",
    "    factor = np.random.uniform(2,4)\n",
    "    lr = 10 ** -factor\n",
    "    \n",
    "    # Randomly select a momentum between 0.85 and 0.99\n",
    "    momentum = np.random.uniform(0.85,0.99)\n",
    "    \n",
    "    values.append((lr, momentum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intermediate Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class object for a PyTorch Tensor dataset\n",
    "\n",
    "class WaterDataset(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        super().__init__()\n",
    "        # Load data to pandas DataFrame\n",
    "        df = pd.read_csv(csv_path)\n",
    "        # Convert data to a NumPy array and assign to self.data\n",
    "        self.data = df.to_numpy()\n",
    "        \n",
    "    # Implement __len__ to return the number of data samples\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        features = self.data[idx, :-1]\n",
    "        # Assign last data column to label\n",
    "        label = self.data[idx,-1]\n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the WaterDataset\n",
    "dataset_train = WaterDataset('water_train.csv')\n",
    "\n",
    "# Create a DataLoader based on dataset_train\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# Get a batch of features and labels\n",
    "features, labels = next(iter(dataloader_train))\n",
    "print(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Define the three linear layers\n",
    "        self.fc1 = nn.Linear(9,16)\n",
    "        self.fc2 = nn.Linear(16,8)\n",
    "        self.fc3 = nn.Linear(8,1)\n",
    "    # Add two batch normalization layers\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.bn2 = nn.BatchNorm1d(8)\n",
    "    \n",
    "    \n",
    "    # Apply He initialization\n",
    "        init.kaiming_uniform_(self.fc1.weight)\n",
    "        init.kaiming_uniform_(self.fc2.weight)\n",
    "        init.kaiming_uniform_(self.fc3.weight, nonlinearity=\"sigmoid\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = nn.functional.elu(x)\n",
    "\n",
    "        # Pass x through the second set of layers\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = nn.functional.elu(x)\n",
    "\n",
    "        x = nn.functional.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "# Set up binary accuracy metric\n",
    "acc = Accuracy(task=\"binary\")\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for features, labels in dataloader_test:\n",
    "        # Get predicted probabilities for test data batch\n",
    "        outputs = net(features)\n",
    "        preds = (outputs >= 0.5).float()\n",
    "        acc(preds, labels.view(-1, 1))\n",
    "\n",
    "# Compute total test accuracy\n",
    "test_accuracy = acc.compute()\n",
    "print(f\"Test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    # Add horizontal flip and rotation\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(45),\n",
    "    transforms.RandomAutocontrast(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((128, 128))\n",
    "])\n",
    "\n",
    "dataset_train = ImageFolder(\n",
    "  \"clouds_train\",\n",
    "  transform=train_transforms,\n",
    ")\n",
    "\n",
    "dataloader_train = DataLoader(\n",
    "  dataset_train, shuffle=True, batch_size=1\n",
    ")\n",
    "\n",
    "image, label = next(iter(dataloader_train))\n",
    "# Reshape the image tensor\n",
    "image = image.squeeze().permute(1, 2, 0) \n",
    "# Display the image\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        # Define feature extractor\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        # Define classifier\n",
    "        self.classifier = nn.Linear(64*16*16, num_classes)\n",
    "    \n",
    "    def forward(self, x):  \n",
    "        # Pass input through feature extractor and classifier\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "net = Net(num_classes=7)\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(3):\n",
    "    running_loss = 0.0\n",
    "    # Iterate over training batches\n",
    "    for images, labels in dataloader_train:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader_train)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics\n",
    "metric_precision = Precision(task=\"multiclass\", num_classes=7, average=\"macro\")\n",
    "metric_recall = Recall(task=\"multiclass\", num_classes=7, average=\"macro\")\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in dataloader_test:\n",
    "        outputs = net(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        metric_precision(preds, labels)\n",
    "        metric_recall(preds, labels)\n",
    "\n",
    "precision = metric_precision.compute()\n",
    "recall = metric_recall.compute()\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define precision metric\n",
    "metric_precision = Precision(\n",
    "    task=\"multiclass\", num_classes=7, average=None\n",
    ")\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in dataloader_test:\n",
    "        outputs = net(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        metric_precision(preds, labels)\n",
    "precision = metric_precision.compute()\n",
    "\n",
    "# Get precision per class\n",
    "precision_per_class = {\n",
    "    k: precision[v].item()\n",
    "    for k, v \n",
    "    in dataset_test.class_to_idx.items()\n",
    "}\n",
    "print(precision_per_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_sequences(df, seq_length):\n",
    "    xs, ys = [], []\n",
    "    # Iterate over data indices\n",
    "    for i in range(len(df) - seq_length):\n",
    "      \t# Define inputs\n",
    "        x = df.iloc[i:(i+seq_length), 1]\n",
    "        # Define target\n",
    "        y = df.iloc[i+seq_length, 1]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Use create_sequences to create inputs and targets\n",
    "X_train, y_train = create_sequences(train_data, 24*4)\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# Create TensorDataset\n",
    "dataset_train = TensorDataset(\n",
    "    torch.from_numpy(X_train).float(),\n",
    "    torch.from_numpy(y_train).float(),\n",
    ")\n",
    "print(len(dataset_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define RNN layer\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=1,\n",
    "            hidden_size=32,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize first hidden state with zeros\n",
    "        h0 = torch.zeros(2, x.size(0), 32)\n",
    "        # Pass x and h0 through recurrent layer\n",
    "        out, _ = self.rnn(x, h0)  \n",
    "        # Pass recurrent layer's last output through linear layer\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        # Define lstm layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=1,\n",
    "            hidden_size=32,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(2, x.size(0), 32)\n",
    "        # Initialize long-term memory\n",
    "        c0 = torch.zeros(2, x.size(0), 32)\n",
    "        # Pass all inputs to lstm layer\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define RNN layer\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=1,\n",
    "            hidden_size=32,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(2, x.size(0), 32)\n",
    "        out, _ = self.gru(x, h0)  \n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "# Set up MSE loss\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(\n",
    "  net.parameters(), lr=0.0001\n",
    ")\n",
    "\n",
    "for epoch in range(3):\n",
    "    for seqs, labels in dataloader_train:\n",
    "        # Reshape model inputs\n",
    "        seqs = seqs.view(16, 96, 1)\n",
    "        # Get model outputs\n",
    "        outputs = net(seqs)\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "    \n",
    "    # Define MSE metric\n",
    "mse = torchmetrics.MeanSquaredError()\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for seqs, labels in dataloader_test:\n",
    "        seqs = seqs.view(32, 96, 1)\n",
    "        # Pass seqs to net and squeeze the result\n",
    "        outputs = net(seqs).squeeze()\n",
    "        mse(outputs, labels)\n",
    "\n",
    "# Compute final metric value\n",
    "test_mse = mse.compute()\n",
    "print(f\"Test MSE: {test_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-input models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "class OmniglotDataset(Dataset):\n",
    "    def __init__(self, transform, samples):\n",
    "\t\t# Assign transform and samples to class attributes\n",
    "        self.transform = transform\n",
    "        self.samples = samples\n",
    "                    \n",
    "    def __len__(self):\n",
    "\t\t# Return number of samples\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "      \t# Unpack the sample at index idx\n",
    "        img_path, alphabet, label = self.samples[idx]\n",
    "        img = Image.open(img_path).convert('L')\n",
    "        # Transform the image \n",
    "        img_transformed = self.transform(img)\n",
    "        return img_transformed, alphabet, label\n",
    "    \n",
    "dataset_train = OmniglotDataset(\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((64, 64)),\n",
    "    ]),\n",
    "    samples=samples,\n",
    ")\n",
    "\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train, shuffle=True, batch_size=3,\n",
    ")\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Define sub-networks as sequential models\n",
    "        self.image_layer = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ELU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16*32*32, 128)\n",
    "        )\n",
    "        self.alphabet_layer = nn.Sequential(\n",
    "            nn.Linear(30, 8),\n",
    "            nn.ELU(), \n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 + 8, 964), \n",
    "        )\n",
    "        \n",
    "    def forward(self, x_image, x_alphabet):\n",
    "\t\t# Pass the x_image and x_alphabet through appropriate layers\n",
    "        x_image = self.image_layer(x_image)\n",
    "        x_alphabet = self.alphabet_layer(x_alphabet)\n",
    "        # Concatenate x_image and x_alphabet\n",
    "        x = torch.cat((x_image, x_alphabet), dim=1)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Output Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the sample at index 100\n",
    "print(samples[100])\n",
    "\n",
    "# Create dataset_train\n",
    "dataset_train = OmniglotDataset(\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "      \ttransforms.Resize((64, 64)),\n",
    "    ]),\n",
    "    samples=samples,\n",
    ")\n",
    "\n",
    "# Create dataloader_train\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train, shuffle=True, batch_size=32,\n",
    ")\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.image_layer = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ELU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16*32*32, 128)\n",
    "        )\n",
    "        # Define the two classifier layers\n",
    "        self.classifier_alpha = nn.Linear(128, 30)\n",
    "        self.classifier_char = nn.Linear(128, 964)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_image = self.image_layer(x)\n",
    "        # Pass x_image through the classifiers and return both results\n",
    "        output_alpha = self.classifier_alpha(x_image)\n",
    "        output_char = self.classifier_char(x_image)\n",
    "        return output_alpha, output_char\n",
    "    \n",
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.05)\n",
    "\n",
    "for epoch in range(1):\n",
    "    for images, labels_alpha, labels_char in dataloader_train:\n",
    "        optimizer.zero_grad()\n",
    "        outputs_alpha, outputs_char = net(images)\n",
    "        # Compute alphabet classification loss\n",
    "        loss_alpha = criterion(outputs_alpha, labels_alpha)\n",
    "        # Compute character classification loss\n",
    "        loss_char = criterion(outputs_char, labels_char)\n",
    "        # Compute total loss\n",
    "        loss = loss_alpha + loss_char\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "def evaluate_model(model):\n",
    "    # Define accuracy metrics\n",
    "    acc_alpha = Accuracy(task=\"multiclass\", num_classes=30)\n",
    "    acc_char = Accuracy(task=\"multiclass\", num_classes=964)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels_alpha, labels_char in dataloader_test:\n",
    "            # Obtain model outputs\n",
    "            outputs_alpha, outputs_char = model(images)\n",
    "            _, pred_alpha = torch.max(outputs_alpha, 1)\n",
    "            _, pred_char = torch.max(outputs_char, 1)\n",
    "\t\t\t# Update both accuracy metrics\n",
    "            acc_alpha(pred_alpha, labels_alpha)\n",
    "            acc_char(pred_char, labels_char)\n",
    "    \n",
    "    print(f\"Alphabet: {acc_alpha.compute()}\")\n",
    "    print(f\"Character: {acc_char.compute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        # Convolution layer with kernel size 3 and stride 1\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1)\n",
    "        # RectiLinear unit activation layer\n",
    "        self.relu = nn.ReLU()\n",
    "        # Max pooling layer with kernel size 2 and stride 2\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Fully connected layer\n",
    "        # Assuming input size of 28x28, the output after conv and pooling layers will be [32, 6, 6]\n",
    "        self.fc = nn.Linear(32 * 6 * 6, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolution\n",
    "        x = self.conv1(x)\n",
    "        # Apply ReLU activation\n",
    "        x = self.relu(x)\n",
    "        # Apply max pooling\n",
    "        x = self.maxpool(x)\n",
    "        # Flatten the output for the fully connected layer\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Apply fully connected layer\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics import Accuracy, Precision, Recall\n",
    "\n",
    "# Get the number of classes\n",
    "classes = train_data.classes\n",
    "num_classes = len(train_data.classes)\n",
    "print(train_dataset.class_to_idx)\n",
    "\n",
    "\n",
    "# Define some relevant variables\n",
    "num_input_channels = 1\n",
    "num_output_channels = 16\n",
    "image_size = train_data[0][0].shape[1]\n",
    "\n",
    "# Define CNN\n",
    "class MultiClassImageClassifier(nn.Module):\n",
    "  \n",
    "    # Define the init method\n",
    "    def __init__(self, num_classes):\n",
    "        super(MultiClassImageClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_input_channels, num_output_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Create a fully connected layer\n",
    "        self.fc = nn.Linear(num_output_channels * (image_size//2)**2, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass inputs through each layer\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "      \n",
    "# Define the training set DataLoader\n",
    "dataloader_train = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=10,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# Define training function\n",
    "def train_model(optimizer, net, num_epochs):\n",
    "    num_processed = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0\n",
    "        num_processed = 0\n",
    "        for features, labels in dataloader_train:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            num_processed += len(labels)\n",
    "        print(f'epoch {epoch}, loss: {running_loss / num_processed}')\n",
    "        \n",
    "    train_loss = running_loss / len(dataloader_train)\n",
    "\n",
    "\n",
    "# Train for 1 epoch\n",
    "net = MultiClassImageClassifier(num_classes)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "train_model(\n",
    "    optimizer=optimizer,\n",
    "    net=net,\n",
    "    num_epochs=1,\n",
    ")\n",
    "\n",
    "# Test the model on the test set\n",
    "              \n",
    "# Define the test set DataLoader\n",
    "dataloader_test = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=10,\n",
    "    shuffle=False,\n",
    ")\n",
    "# Define the metrics\n",
    "accuracy_metric = Accuracy(task='multiclass', num_classes=num_classes)\n",
    "precision_metric = Precision(task='multiclass', num_classes=num_classes, average=None)\n",
    "recall_metric = Recall(task='multiclass', num_classes=num_classes, average=None)\n",
    "\n",
    "# Run model on test set\n",
    "net.eval()\n",
    "predicted = []\n",
    "for i, (features, labels) in enumerate(dataloader_test):\n",
    "    output = net.forward(features.reshape(-1, 1, image_size, image_size))\n",
    "    cat = torch.argmax(output, dim=-1)\n",
    "    predicted.extend(cat.tolist())\n",
    "    accuracy_metric(cat, labels)\n",
    "    precision_metric(cat, labels)\n",
    "    recall_metric(cat, labels)\n",
    "\n",
    "# Compute the metrics\n",
    "accuracy = accuracy_metric.compute().item()\n",
    "precision = precision_metric.compute().tolist()\n",
    "recall = recall_metric.compute().tolist()\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision (per class):', precision)\n",
    "print('Recall (per class):', recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of innput channels\n",
    "from torchvision.transforms import functional\n",
    "\n",
    "image = PIL.Image.open(\"dog.png\")\n",
    "num_channels = functional.get_image_num_channels(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryImageClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryImageClassifier, self).__init__()\n",
    "        \n",
    "        # Create a convolutional layer\n",
    "        # Input: 3 RGB channels, 1 for grayscale, 4 for transparency includes alpha channel\n",
    "        # Output 16 channels, Kernel 3x3 matrix, padding 1 pixel around the border, stride is the number of pixels shifted\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        # A non-linear activation function\n",
    "        self.relu = nn.ReLU()\n",
    "        # Reduces to half\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Tensors flattened into 1-D vector\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Create a fully connected layer\n",
    "        # Input feature maps (16) x height (32) x weidth (32) 32 is half of the original image size after Maxpool\n",
    "        # Output 1 is for a single class (binay)\n",
    "        self.fc = nn.Linear(16*32*32, 1)\n",
    "            \n",
    "        # Create an activation function- for binary use Sigmoid\n",
    "        self.sigmoid = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model\n",
    "model = CNNModel()\n",
    "print(\"Original model: \", model)\n",
    "\n",
    "# Create a new convolutional layer\n",
    "conv2 = nn.Conv2d(in_channels=16, kernel_size=3, out_channels=32, stride=1, padding=1)\n",
    "\n",
    "# Append the new layer to the model\n",
    "model.add_module('conv2', conv2)\n",
    "print(\"Extended model: \", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryImageClassification(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(BinaryImageClassification, self).__init__()\n",
    "    # Create a convolutional block\n",
    "    self.conv_block = nn.Sequential(\n",
    "      nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "      nn.ReLU(),\n",
    "    )\n",
    "    \n",
    "  def forward(self, x):\n",
    "    # Pass inputs through the convolutional block\n",
    "    x = self.conv_block(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Multi Class classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassImageClassifier(nn.Module):\n",
    "  \n",
    "    # Define the init method\n",
    "    def __init__(self, num_classes):\n",
    "        super(MultiClassImageClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Create a fully connected layer\n",
    "        self.fc = nn.Linear(16*32*32, num_classes)\n",
    "        \n",
    "        # Create an activation function\n",
    "        self.softmax = nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawing boundry boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert bbox into tensors\n",
    "bbox_tensor = torch.tensor(bbox)\n",
    "\n",
    "# Add a new batch dimension\n",
    "bbox_tensor = bbox_tensor.unsqueeze(0)\n",
    "\n",
    "# Resize the image and transform to tensor\n",
    "transform = transforms.Compose([\n",
    "  transforms.Resize(224),\n",
    "  transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Apply transform to image\n",
    "image_tensor = transform(image)\n",
    "print(image_tensor)\n",
    "\n",
    "# Import draw_bounding_boxes\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "\n",
    "# Define the bounding box coordinates\n",
    "bbox = [x_min, y_min, x_max, y_max]\n",
    "bbox_tensor = torch.tensor(bbox).unsqueeze(0)\n",
    "\n",
    "# Implement draw_bounding_boxes\n",
    "img_bbox = draw_bounding_boxes(image_tensor, bbox_tensor, width=3, colors=\"red\")\n",
    "\n",
    "# Tranform tensors to image\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage()\n",
    "])\n",
    "plt.imshow(transform(img_bbox))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model's prediction\n",
    "with torch.no_grad():\n",
    "    output = model(test_image)\n",
    "\n",
    "# Extract boxes from the output\n",
    "boxes = output[0][\"boxes\"]\n",
    "\n",
    "# Extract scores from the output\n",
    "scores = output[0][\"scores\"]\n",
    "\n",
    "print(boxes, scores)\n",
    "\n",
    "# Import nms\n",
    "from torchvision.ops import nms\n",
    "\n",
    "# Set the IoU threshold\n",
    "iou_threshold = 0.5\n",
    "\n",
    "# Apply non-max suppression\n",
    "box_indices = nms(boxes=boxes, scores=scores, iou_threshold=iou_threshold)\n",
    "\n",
    "# Filter boxes\n",
    "filtered_boxes = boxes[box_indices]\n",
    "\n",
    "print(\"Filtered Boxes:\", filtered_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained weights\n",
    "vgg_model = vgg16(weights=VGG16_Weights.DEFAULT)\n",
    "\n",
    "# Extract the input dimension\n",
    "input_dim = nn.Sequential(*list(vgg_model.classifier.children()))[0].in_features\n",
    "\n",
    "# Create a backbone with convolutional layers\n",
    "backbone = nn.Sequential(*list(vgg_model.features.children()))\n",
    "\n",
    "# Print the backbone model\n",
    "print(backbone)\n",
    "\n",
    "# Create a variable with the number of classes\n",
    "num_classes = 2\n",
    "    \n",
    "# Create a sequential block\n",
    "classifier = nn.Sequential(\n",
    "\t# Create a linear layer with input features\n",
    "\tnn.Linear(input_dim, 512),\n",
    "\tnn.ReLU(),\n",
    "\t# Add the output dimension to the classifier\n",
    "\tnn.Linear(512, num_classes),\n",
    ")\n",
    "\n",
    "# Define the number of coordinates\n",
    "num_coordinates = 4\n",
    "\n",
    "bb = nn.Sequential(  \n",
    "\t# Add input and output dimensions\n",
    "\tnn.Linear(input_dim, 32),\n",
    "\tnn.ReLU(),\n",
    "\t# Add the output for the last regression layer\n",
    "\tnn.Linear(32, num_coordinates),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import AnchorGenerator\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "# Configure anchor size\n",
    "anchor_sizes = ((32, 64, 128),)\n",
    "\n",
    "# Configure aspect ratio\n",
    "aspect_ratios = ((0.5, 1.0, 2.0),)\n",
    "\n",
    "# Instantiate AnchorGenerator\n",
    "rpn_anchor_generator = AnchorGenerator(sizes=anchor_sizes, aspect_ratios=aspect_ratios)\n",
    "\n",
    "\n",
    "# Import MultiScaleRoIAlign\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "\n",
    "# Instantiate RoI pooler\n",
    "roi_pooler = MultiScaleRoIAlign(\n",
    "\tfeatmap_names=[\"0\"],\n",
    "\toutput_size=7,\n",
    "\tsampling_ratio=2,\n",
    ")\n",
    "\n",
    "mobilenet = torchvision.models.mobilenet_v2(weights=\"DEFAULT\")\n",
    "backbone = nn.Sequential(*list(mobilenet.features.children()))\n",
    "backbone.out_channels = 1280\n",
    "\n",
    "# Create Faster R-CNN model\n",
    "model = FasterRCNN(\n",
    "\tbackbone=backbone,\n",
    "\tnum_classes=2,\n",
    "\tanchor_generator=anchor_generator,\n",
    "\tbox_roi_pool=roi_pooler,\n",
    ")\n",
    "\n",
    "# Implement the RPN classification loss function\n",
    "rpn_cls_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Implement the RPN regression loss function\n",
    "rpn_reg_criterion = nn.MSELoss()\n",
    "\n",
    "# Implement the R-CNN classification Loss function\n",
    "rcnn_cls_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Implement the R-CNN regression loss function\n",
    "rcnn_reg_criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and Load a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), \"ModelCNN.pth\")\n",
    "\n",
    "# Create a new model\n",
    "loaded_model = ManufacturingCNN()\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model.load_state_dict(torch.load('ModelCNN.pth'))\n",
    "print(loaded_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Tourchvision models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import resnet18 model\n",
    "from torchvision.models import (resnet18, ResNet18_Weights)\n",
    "\n",
    "# Initialize model with default weights\n",
    "weights = ResNet18_Weights.DEFAULT\n",
    "model = resnet18(weights=weights)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize the transforms\n",
    "transform = weights.transforms()\n",
    "\n",
    "# Apply preprocessing transforms\n",
    "batch = preprocess(img).unsqueeze(0)\n",
    "\n",
    "# Apply model with softmax layer\n",
    "prediction = model(batch).squeeze(0).softmax(0)\n",
    "\n",
    "# Apply argmax\n",
    "class_id = prediction.argmax().item()\n",
    "score = prediction[class_id].item()\n",
    "category_name = weights.meta[\"categories\"][class_id]\n",
    "print(category_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mask image\n",
    "mask = Image.open(\"annotations/Egyptian_Mau_123.png\")\n",
    "\n",
    "# Transform mask to tensor\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "mask_tensor = transform(mask)\n",
    "\n",
    "# Create binary mask\n",
    "binary_mask = torch.where(\n",
    "    mask_tensor ==1/255, \n",
    "    torch.tensor(1.0),\n",
    "    torch.tensor(0.0),\n",
    ")\n",
    "\n",
    "# Print unique mask values\n",
    "print(binary_mask.unique())\n",
    "\n",
    "# Load image and transform to tensor\n",
    "image = Image.open(\"images/Egyptian_Mau_123.jpg\")\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "image_tensor = transform(image)\n",
    "\n",
    "# Segment object out of the image\n",
    "object_tensor = binary_mask * image_tensor\n",
    "\n",
    "# Convert segmented object to image and display\n",
    "to_pil_image = transforms.ToPILImage()\n",
    "object_image = to_pil_image(object_tensor)\n",
    "plt.imshow(object_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import maskrcnn_resnet50_fpn\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "\n",
    "# Load a pre-trained Mask R-CNN model\n",
    "model = maskrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Load an image and convert to a tensor\n",
    "image = Image.open(\"two_cats.jpg\")\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "image_tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    prediction = model(image_tensor)\n",
    "    print(prediction)\n",
    "    \n",
    "prediction[0][\"labels\"]\n",
    "print(class_names[17], class_names[15], class_names[64])\n",
    "prediction[0][\"scores\"]\n",
    "\n",
    "# Extract masks and labels from prediction\n",
    "masks = prediction[0][\"masks\"]\n",
    "labels = prediction[0][\"labels\"]\n",
    "\n",
    "# Plot image with two overlaid masks\n",
    "for i in range(2):\n",
    "    plt.imshow(image)\n",
    "    # Overlay the i-th mask on top of the image\n",
    "    plt.imshow(masks[i,0], cmap=\"jet\", alpha=0.5)\n",
    "    plt.title(f\"Object: {class_names[labels[i]]}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.enc1 = self.conv_block(in_channels, 64)\n",
    "        self.enc2 = self.conv_block(64, 128)\n",
    "        self.enc3 = self.conv_block(128, 256)\n",
    "        self.enc4 = self.conv_block(256, 512)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        \n",
    "        # Define the decoder blocks\n",
    "        self.dec1 = self.conv_block(512, 256)\n",
    "        self.dec2 = self.conv_block(256, 128)\n",
    "        self.dec3 = self.conv_block(128, 64)\n",
    "\n",
    "        self.out = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "        \n",
    "def forward(self, x):\n",
    "    x1 = self.enc1(x)\n",
    "    x2 = self.enc2(self.pool(x1))\n",
    "    x3 = self.enc3(self.pool(x2))\n",
    "    x4 = self.enc4(self.pool(x3))\n",
    "\n",
    "    x = self.upconv3(x4)\n",
    "    x = torch.cat([x, x3], dim=1)\n",
    "    x = self.dec1(x)\n",
    "\n",
    "    x = self.upconv2(x)\n",
    "    x = torch.cat([x, x2], dim=1)\n",
    "    x = self.dec2(x)\n",
    "\n",
    "    # Define the last decoder block with skip connections\n",
    "    x = self.upconv1(x)\n",
    "    x = torch.cat([x, x1], dim=1)\n",
    "    x = self.dec3(x)\n",
    "\n",
    "    return self.out(x)\n",
    "\n",
    "# Load model\n",
    "model = UNet()\n",
    "model.eval()\n",
    "\n",
    "# Load and transform image\n",
    "image = Image.open(\"car.jpg\")\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "image_tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "# Predict segmentation mask\n",
    "with torch.no_grad():\n",
    "    prediction = model(image_tensor).squeeze(0)\n",
    "\n",
    "# Display mask\n",
    "plt.imshow(prediction[1, :, :])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Panoptic Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instantiate the model\n",
    "model = UNet()\n",
    "\n",
    "# Produce semantic masks for the input image\n",
    "with torch.no_grad():\n",
    "    semantic_masks = model(image_tensor)\n",
    "\n",
    "# Choose highest-probability class for each pixel\n",
    "semantic_mask = torch.argmax(semantic_masks, dim=1)\n",
    "\n",
    "# Display the mask\n",
    "plt.imshow(semantic_mask.squeeze(0))\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Instantiate model and produce instance masks\n",
    "model = MaskRCNN()\n",
    "with torch.no_grad():\n",
    "    instance_masks = model(image_tensor)[0][\"masks\"]\n",
    "\n",
    "# Initialize panoptic mask as semantic_mask\n",
    "panoptic_mask = torch.clone(semantic_mask)\n",
    "\n",
    "# Iterate over instance masks\n",
    "instance_id = 3\n",
    "for mask in instance_masks:\n",
    "    # Set panoptic mask to instance_id where mask > 0.5\n",
    "    panoptic_mask[mask > 0.5] = instance_id\n",
    "    instance_id += 1\n",
    "    \n",
    "# Display panoptic mask\n",
    "plt.imshow(panoptic_mask.squeeze(0))\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Adversarial Networks (GANs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_block(in_dim, out_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_dim, out_dim),\n",
    "        nn.BatchNorm1d(out_dim),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        # Define generator block\n",
    "        self.generator = nn.Sequential(\n",
    "            gen_block(in_dim, 256),\n",
    "            gen_block(256, 512),\n",
    "            gen_block(512, 1024),\n",
    "          \t# Add linear layer\n",
    "            nn.Linear(1024, out_dim),\n",
    "            # Add activation\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "      \t# Pass input through generator\n",
    "        return self.generator(x)\n",
    "    \n",
    "def disc_block(in_dim, out_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_dim, out_dim),\n",
    "        nn.LeakyReLU(0.2)\n",
    "    )\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, im_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            disc_block(im_dim, 1024),\n",
    "            disc_block(1024, 512),\n",
    "            # Define last discriminator block\n",
    "            disc_block(512, 256),\n",
    "            # Add a linear layer\n",
    "            nn.Linear(256,1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward method\n",
    "        return self.disc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Convolutional GAM\n",
    "gen, the generator model\n",
    "disc, the discriminator model\n",
    "num_images, the number of images in batch\n",
    "z_dim, the size of the input random noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dc_gen_block(in_dim, out_dim, kernel_size, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_dim, out_dim, kernel_size, stride=stride),\n",
    "        nn.BatchNorm2d(out_dim),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "    \n",
    "class DCGenerator(nn.Module):\n",
    "    def __init__(self, in_dim, kernel_size=4, stride=2):\n",
    "        super(DCGenerator, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.gen = nn.Sequential(\n",
    "            dc_gen_block(in_dim, 1024, kernel_size, stride),\n",
    "            dc_gen_block(1024, 512, kernel_size, stride),\n",
    "            # Add last generator block\n",
    "            dc_gen_block(512, 256, kernel_size, stride),\n",
    "            # Add transposed convolution\n",
    "            nn.ConvTranspose2d(256, 3, kernel_size, stride=stride),\n",
    "            # Add tanh activation\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(len(x), self.in_dim, 1, 1)\n",
    "        return self.gen(x)\n",
    "    \n",
    "def dc_disc_block(in_dim, out_dim, kernel_size, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_dim, out_dim, kernel_size, stride=stride),\n",
    "        nn.BatchNorm2d(out_dim),\n",
    "        nn.LeakyReLU(0.2),\n",
    "    )\n",
    "    \n",
    "class DCDiscriminator(nn.Module):\n",
    "    def __init__(self, kernel_size=4, stride=2):\n",
    "        super(DCDiscriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "          \t# Add first discriminator block\n",
    "            dc_disc_block(3, 512, kernel_size, stride),\n",
    "            dc_disc_block(512, 1024, kernel_size, stride),\n",
    "          \t# Add a convolution\n",
    "            nn.Conv2d(1024, 1, kernel_size, stride=stride),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through sequential block\n",
    "        x = self.disc(x)\n",
    "        return x.view(len(x), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_loss(gen, disc, criterion, num_images, z_dim):\n",
    "    # Define random noise\n",
    "    noise = torch.randn(num_images, z_dim)\n",
    "    # Generate fake image\n",
    "    fake = gen(noise)\n",
    "    # Get discriminator's prediction on the fake image\n",
    "    disc_pred = disc(fake)\n",
    "    # Compute generator loss\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    gen_loss = criterion(disc_pred, torch.ones_like(disc_pred))\n",
    "    return gen_loss\n",
    "\n",
    "def disc_loss(gen, disc, real, num_images, z_dim):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    noise = torch.randn(num_images, z_dim)\n",
    "    fake = gen(noise)\n",
    "    # Get discriminator's predictions for fake images\n",
    "    disc_pred_fake = disc(fake)\n",
    "    # Calculate the fake loss component\n",
    "    fake_loss = criterion(disc_pred_fake, torch.zeros_like(disc_pred_fake))\n",
    "    # Get discriminator's predictions for real images\n",
    "    disc_pred_real = disc(real)\n",
    "    # Calculate the real loss component\n",
    "    real_loss = criterion(disc_pred_real, torch.ones_like(disc_pred_real))\n",
    "    disc_loss = (real_loss + fake_loss) / 2\n",
    "    return disc_loss\n",
    "\n",
    "for epoch in range(1):\n",
    "    for real in dataloader:\n",
    "        cur_batch_size = len(real)\n",
    "        \n",
    "        disc_opt.zero_grad()\n",
    "        # Calculate discriminator loss\n",
    "        disc_loss = disc_loss(gen, disc, real, cur_batch_size, z_dim=16)\n",
    "        # Compute gradients\n",
    "        disc_loss.backward()\n",
    "        disc_opt.step()\n",
    "\n",
    "        gen_opt.zero_grad()\n",
    "        # Calculate generator loss\n",
    "        gen_loss = gen_loss(gen, disc, cur_batch_size, z_dim=16)\n",
    "        # Compute generator gradients\n",
    "        gen_loss.backward()\n",
    "        gen_opt.step()\n",
    "\n",
    "        print(f\"Generator loss: {gen_loss}\")\n",
    "        print(f\"Discriminator loss: {disc_loss}\")\n",
    "        break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images_to_generate = 5\n",
    "# Create random noise tensor\n",
    "noise = torch.randn(num_images_to_generate, 16)\n",
    "\n",
    "# Generate images\n",
    "with torch.no_grad():\n",
    "    fake = gen(noise)\n",
    "print(f\"Generated tensor shape: {fake.shape}\")\n",
    "    \n",
    "for i in range(num_images_to_generate):\n",
    "    # Slice fake to select i-th image\n",
    "    image_tensor = fake[i, :, :, :]\n",
    "    # Permute the image dimensions\n",
    "    image_tensor_permuted = image_tensor.permute(1, 2, 0)\n",
    "    plt.imshow(image_tensor_permuted)\n",
    "    plt.show()\n",
    "    \n",
    "# Import FrechetInceptionDistance\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "\n",
    "# Instantiate FID\n",
    "fid = FrechetInceptionDistance(feature=64)\n",
    "\n",
    "# Update FID with real images\n",
    "fid.update((fake * 255).to(torch.uint8), real=False)\n",
    "fid.update((real * 255).to(torch.uint8), real=True)\n",
    "\n",
    "# Compute the metric\n",
    "fid_score = fid.compute()\n",
    "print(fid_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary functions\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from nltk.probability import FreqDist\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "text = \"In the city of Dataville, a data analyst named Alex explores hidden insights within vast data. With determination, Alex uncovers patterns, cleanses the data, and unlocks innovation. Join this adventure to unleash the power of data-driven decisions.\"\n",
    "\n",
    "# Initialize the tokenizer and tokenize the text\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokens = tokenizer(text)\n",
    "\n",
    "threshold = 1\n",
    "# Remove rare words and print common tokens\n",
    "freq_dist = FreqDist(tokens)\n",
    "common_tokens = [token for token in tokens if freq_dist[token] > threshold]\n",
    "print(common_tokens)\n",
    "\n",
    "# Initialize and tokenize the text\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokens = tokenizer(text)\n",
    "\n",
    "# Remove any stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "# Perform stemming on the filtered tokens\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "print(stemmed_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = ['Fiction','Non-fiction','Biography', 'Children','Mystery']\n",
    "\n",
    "# Define the size of the vocabulary\n",
    "vocab_size = len(genres)\n",
    "\n",
    "# Create one-hot vectors\n",
    "one_hot_vectors = torch.eye(vocab_size)\n",
    "\n",
    "# Create a dictionary mapping genres to their one-hot vectors\n",
    "one_hot_dict = {genre: one_hot_vectors[i] for i, genre in enumerate(genres)}\n",
    "\n",
    "for genre, vector in one_hot_dict.items():\n",
    "    print(f'{genre}: {vector.numpy()}')\n",
    "    \n",
    "# Import from sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "titles = ['The Great Gatsby','To Kill a Mockingbird','1984','The Catcher in the Rye','The Hobbit', 'Great Expectations']\n",
    "\n",
    "# Initialize Bag-of-words with the list of book titles\n",
    "vectorizer = CountVectorizer()\n",
    "bow_encoded_titles = vectorizer.fit_transform(titles)\n",
    "\n",
    "# Extract and print the first five features\n",
    "print(vectorizer.get_feature_names_out()[:5])\n",
    "print(bow_encoded_titles.toarray()[0, :5])\n",
    "\n",
    "# Importing TF-IDF from sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF encoding vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_encoded_descriptions = vectorizer.fit_transform(descriptions)\n",
    "\n",
    "# Extract and print the first five features\n",
    "print(vectorizer.get_feature_names_out()[:5])\n",
    "print(tfidf_encoded_descriptions.toarray()[0, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Initialize the tokenizer and stemmer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "stemmer = PorterStemmer() \n",
    "\n",
    "# Complete the function to preprocess sentences\n",
    "def preprocess_sentences(sentences):\n",
    "    processed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        tokens = tokenizer(sentence)\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "        processed_sentences.append(' '.join(tokens))\n",
    "    return processed_sentences\n",
    "\n",
    "processed_shakespeare = preprocess_sentences(shakespeare)\n",
    "print(processed_shakespeare[:5]) \n",
    "\n",
    "# Define your Dataset class\n",
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Complete the encoding function\n",
    "def encode_sentences(sentences):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(sentences)\n",
    "    return X.toarray(), vectorizer\n",
    "    \n",
    "# Complete the text processing pipeline\n",
    "def text_processing_pipeline(sentences):\n",
    "    processed_sentences = preprocess_sentences(sentences)\n",
    "    encoded_sentences, vectorizer = encode_sentences(processed_sentences)\n",
    "    dataset = ShakespeareDataset(encoded_sentences)\n",
    "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "    return dataloader, vectorizer\n",
    "\n",
    "dataloader, vectorizer = text_processing_pipeline(processed_shakespeare)\n",
    "\n",
    "# Print the vectorizer's feature names and the first 10 components of the first item\n",
    "print(vectorizer.get_feature_names_out()[:10]) \n",
    "print(next(iter(dataloader))[0, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map a unique index to each word\n",
    "words = [\"This\", \"book\", \"was\", \"fantastic\", \"I\", \"really\", \"love\", \"science\", \"fiction\", \"but\", \"the\", \"protagonist\", \"was\", \"rude\", \"sometimes\"]\n",
    "word_to_idx = {word: i for i, word in enumerate(words)}\n",
    "\n",
    "# Convert word_to_idx to a tensor\n",
    "inputs = torch.LongTensor([word_to_idx[w] for w in words])\n",
    "\n",
    "# Initialize embedding layer with ten dimensions\n",
    "embedding = nn.Embedding(num_embeddings=len(words), embedding_dim=10)\n",
    "\n",
    "# Pass the tensor to the embedding layer\n",
    "output = embedding(inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(TextClassificationCNN, self).__init__()\n",
    "        # Initialize the embedding layer \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.conv = nn.Conv1d(embed_dim, embed_dim, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc = nn.Linear(embed_dim, 2)\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text).permute(0, 2, 1)\n",
    "        # Pass the embedded text through the convolutional layer and apply a ReLU\n",
    "        conved = F.relu(self.conv(embedded))\n",
    "        conved = conved.mean(dim=2) \n",
    "        return self.fc(conved)\n",
    "    \n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for sentence, label in data:     \n",
    "        # Clear the gradients\n",
    "        model.zero_grad()\n",
    "        sentence = torch.LongTensor([word_to_ix.get(w, 0) for w in sentence]).unsqueeze(0) \n",
    "        label = torch.LongTensor([int(label)])\n",
    "        outputs = model(sentence)\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "print('Training complete!')\n",
    "\n",
    "book_reviews = [\n",
    "    \"I love this book\".split(),\n",
    "    \"I do not like this book\".split()\n",
    "]\n",
    "for review in book_reviews:\n",
    "    # Convert the review words into tensor form\n",
    "    input_tensor = torch.tensor([word_to_ix[w] for w in review], dtype=torch.long).unsqueeze(0) \n",
    "    # Get the model's output\n",
    "    outputs = model(input_tensor)\n",
    "    # Find the index of the most likely sentiment category\n",
    "    _, predicted_label = torch.max(outputs.data, 1)\n",
    "    # Convert the predicted label into a sentiment string\n",
    "    sentiment = \"Positive\" if predicted_label.item() == 1 else \"Negative\"\n",
    "    print(f\"Book Review: {' '.join(review)}\")\n",
    "    print(f\"Sentiment: {sentiment}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the RNN class\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = out[:, -1, :] \n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "rnn_model = RNNModel(input_size, hidden_size, num_layers, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn_model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model for ten epochs and zero the gradients\n",
    "for epoch in range(10): \n",
    "    optimizer.zero_grad()\n",
    "    outputs = rnn_model(X_train_seq)\n",
    "    loss = criterion(outputs, y_train_seq)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch: {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LSTM and the output layer with parameters\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = out[:, -1, :] \n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Initialize model with required parameters\n",
    "lstm_model = LSTMModel(input_size, hidden_size, num_layers, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model by passing the correct parameters and zeroing the gradient\n",
    "for epoch in range(10): \n",
    "    optimizer.zero_grad()\n",
    "    outputs = lstm_model(X_train_seq)\n",
    "    loss = criterion(outputs, y_train_seq)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch: {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the GRU model\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)       \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size) \n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = out[:, -1, :] \n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "gru_model = GRUModel(input_size, hidden_size, num_layers, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(gru_model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model and backpropagate the loss after initialization\n",
    "for epoch in range(15): \n",
    "    optimizer.zero_grad()\n",
    "    outputs = gru_model(X_train_seq)\n",
    "    loss = criterion(outputs, y_train_seq)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch: {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the metrics\n",
    "accuracy = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "precision = Precision(task=\"multiclass\", num_classes=num_classes)\n",
    "recall = Recall(task=\"multiclass\", num_classes=num_classes)\n",
    "f1 = F1Score(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "# Generate the predictions\n",
    "outputs = rnn_model(X_test_seq)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy_score = accuracy(predicted, y_test_seq)\n",
    "precision_score = precision(predicted, y_test_seq)\n",
    "recall_score = recall(predicted, y_test_seq)\n",
    "f1_score = f1(predicted, y_test_seq)\n",
    "print(\"RNN Model - Accuracy: {}, Precision: {}, Recall: {}, F1 Score: {}\".format(accuracy_score, precision_score, recall_score, f1_score))\n",
    "\n",
    "\n",
    "# Calculate metrics for the LSTM model\n",
    "accuracy_1 = accuracy(y_pred_lstm, y_test)\n",
    "precision_1 = precision(y_pred_lstm, y_test)\n",
    "recall_1 = recall(y_pred_lstm, y_test)\n",
    "f1_1 = f1(y_pred_lstm, y_test)\n",
    "print(\"LSTM Model - Accuracy: {}, Precision: {}, Recall: {}, F1 Score: {}\".format(accuracy_1, precision_1, recall_1, f1_1))\n",
    "\n",
    "# Calculate metrics for the GRU model\n",
    "accuracy_2 = accuracy(y_pred_gru, y_test)\n",
    "precision_2 = precision(y_pred_gru, y_test)\n",
    "recall_2 = recall(y_pred_gru, y_test)\n",
    "f1_2 = f1(y_pred_gru, y_test)\n",
    "print(\"GRU Model - Accuracy: {}, Precision: {}, Recall: {}, F1 Score: {}\".format(accuracy_2, precision_2, recall_2, f1_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include an RNN layer and linear layer in RNNmodel class\n",
    "class RNNmodel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNmodel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "      h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "      out, _ = self.rnn(x, h0)  \n",
    "      out = self.fc(out[:, -1, :])  \n",
    "      return out\n",
    "\n",
    "# Instantiate the RNN model\n",
    "model = RNNmodel(len(chars), 16, len(chars))\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}/100, Loss: {loss.item()}')\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "test_input = char_to_ix['r']\n",
    "test_input = nn.functional.one_hot(torch.tensor(test_input).view(-1, 1), num_classes=len(chars)).float()\n",
    "predicted_output = model(test_input)\n",
    "predicted_char_ix = torch.argmax(predicted_output, 1).item()\n",
    "print(f\"Test Input: 'r', Predicted Output: '{ix_to_char[predicted_char_ix]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Adversarial Networks for text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(nn.Linear(seq_length, seq_length), nn.Sigmoid())\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Define the discriminator networks\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(nn.Linear(seq_length,1), nn.Sigmoid())\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_gen = torch.optim.Adam(generator.parameters(), lr=0.001)\n",
    "optimizer_disc = torch.optim.Adam(discriminator.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for real_data in data:\n",
    "        # Unsqueezing real_data and prevent gradient recalculations\n",
    "        real_data = real_data.unsqueeze(0)\n",
    "        noise = torch.rand((1, seq_length))\n",
    "        fake_data = generator(noise)\n",
    "        disc_real = discriminator(real_data)\n",
    "        disc_fake = discriminator(fake_data.detach())\n",
    "        loss_disc = criterion(disc_real, torch.ones_like(disc_real)) + criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        optimizer_disc.zero_grad()\n",
    "        loss_disc.backward()\n",
    "        optimizer_disc.step()\n",
    "\n",
    "        # Train the generator\n",
    "        disc_fake = discriminator(fake_data)\n",
    "        loss_gen = criterion(disc_fake, torch.ones_like(disc_fake))\n",
    "        optimizer_gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        optimizer_gen.step()\n",
    "\n",
    "    if (epoch+1) % print_every == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}:\\t Generator loss: {loss_gen.item()}\\t Discriminator loss: {loss_disc.item()}\")\n",
    "\n",
    "print(\"\\nReal data: \")\n",
    "print(data[:5])\n",
    "\n",
    "print(\"\\nGenerated data: \")\n",
    "for _ in range(5):\n",
    "    noise = torch.rand((1, seq_length))\n",
    "    generated_data = generator(noise)\n",
    "    # Detach the tensor and print data\n",
    "    print(torch.round(generated_data).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Initialize the pre-trained model\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "seed_text = \"Once upon a time\"\n",
    "\n",
    "# Encode the seed text to get input tensors\n",
    "input_ids = tokenizer.encode(seed_text, return_tensors='pt')\n",
    "\n",
    "# Generate text from the model\n",
    "output = model.generate(input_ids, max_length=100, temperature=0.7, no_repeat_ngram_size=2, pad_token_id=tokenizer.eos_token_id) \n",
    "\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Initalize tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "input_prompt = \"translate English to French: 'Hello, how are you?'\"\n",
    "\n",
    "# Encode the input prompt using the tokenizer\n",
    "input_ids = tokenizer.encode(input_prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate the translated ouput\n",
    "output = model.generate(input_ids, max_length=50)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated text:\",generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.text import BLEUScore, ROUGEScore\n",
    "\n",
    "reference_text = \"Once upon a time, there was a little girl who lived in a village near the forest.\"\n",
    "generated_text = \"Once upon a time, the world was a place of great beauty and great danger. The world of the gods was the place where the great gods were born, and where they were to live.\"\n",
    "\n",
    "# Initialize BLEU and ROUGE scorers\n",
    "bleu = BLEUScore()\n",
    "rouge = ROUGEScore()\n",
    "\n",
    "# Calculate the BLEU and ROUGE scores\n",
    "bleu_score = bleu([generated_text], [[reference_text]])\n",
    "rouge_score = rouge([generated_text], [[reference_text]])\n",
    "\n",
    "# Print the BLEU and ROUGE scores\n",
    "print(\"BLEU Score:\", bleu_score.item())\n",
    "print(\"ROUGE Score:\", rouge_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "texts = [\"I love this!\",\n",
    "\"This is terrible.\",\n",
    "\"Amazing experience!\",\n",
    "\"Not my cup of tea.\"]\n",
    "labels = [1, 0, 1, 0]\n",
    "\n",
    "# Load the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Tokenize your data and return PyTorch tensors\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=32)\n",
    "inputs[\"labels\"] = torch.tensor(labels)\n",
    "\n",
    "# Setup the optimizer using model parameters\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00001)\n",
    "model.train()\n",
    "for epoch in range(2):\n",
    "    outputs = model(**inputs)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "text = \"I had an awesome day!\"\n",
    "\n",
    "# Tokenize the text and return PyTorch tensors\n",
    "input_eval = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=32)\n",
    "outputs_eval = model(**input_eval)\n",
    "\n",
    "# Convert the output logits to probabilities\n",
    "predictions = torch.nn.functional.softmax(outputs_eval.logits, dim=-1)\n",
    "\n",
    "# Display the sentiments\n",
    "predicted_label = 'positive' if torch.argmax(predictions) > 0 else 'negative'\n",
    "print(f\"Text: {text}\\nSentiment: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_size, heads, num_layers, dropout):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        # Initialize the encoder \n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=embed_size, nhead=heads),\n",
    "            num_layers=num_layers)\n",
    "        # Define the fully connected layer\n",
    "        self.fc = nn.Linear(embed_size, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the transformer encoder \n",
    "        x = self.encoder(x)\n",
    "        x = x.mean(dim=1) \n",
    "        return self.fc(x)\n",
    "\n",
    "model = TransformerEncoder(embed_size=512, heads=8, num_layers=3, dropout=0.5)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(5):  \n",
    "    for sentence, label in zip(train_sentences, train_labels):\n",
    "        # Split the sentences into tokens and stack the embeddings\n",
    "        tokens = sentence.split()\n",
    "        data = torch.stack([token_embeddings[token] for token in tokens], dim=1)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, torch.tensor([label]))\n",
    "        # Zero the gradients and perform a backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "def predict(sentence):\n",
    "    model.eval()\n",
    "    # Deactivate the gradient computations and get the sentiment prediction.\n",
    "    with torch.no_grad():\n",
    "        tokens = sentence.split()\n",
    "        data = torch.stack([token_embeddings.get(token, torch.rand((1, 512))) for token in tokens], dim=1)\n",
    "        output = model(data)\n",
    "        predicted = torch.argmax(output, dim=1)\n",
    "        return \"Positive\" if predicted.item() == 1 else \"Negative\"\n",
    "\n",
    "sample_sentence = \"This product can be better\"\n",
    "print(f\"'{sample_sentence}' is {predict(sample_sentence)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a RNN mdel with attention (predicting the next word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['the cat sat on the mat',\n",
    " 'dogs are very loyal animals',\n",
    " 'parrots are colorful and noisy',\n",
    " 'whales are the largest mammals']\n",
    "vocab = set(' '.join(data).split())\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "ix_to_word = {i: word for word, i in word_to_ix.items()}\n",
    "pairs = [sentence.split() for sentence in data]\n",
    "input_data = [[word_to_ix[word] for word in sentence[:-1]] for sentence in pairs]\n",
    "target_data = [word_to_ix[sentence[-1]] for sentence in pairs]\n",
    "inputs = [torch.tensor(seq, dtype=torch.long) for seq in input_data]\n",
    "targets = torch.tensor(target_data, dtype=torch.long)\n",
    "embedding_dim = 10\n",
    "hidden_dim = 16\n",
    "\n",
    "class RNNWithAttentionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNNWithAttentionModel, self).__init__()\n",
    "        # Create an embedding layer for the vocabulary\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        # Apply a linear transformation to get the attention scores\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        out, _ = self.rnn(x)\n",
    "        #  Get the attention weights\n",
    "        attn_weights = torch.nn.functional.softmax(self.attention(out).squeeze(2), dim=1)\n",
    "        # Compute the context vector \n",
    "        context = torch.sum(attn_weights.unsqueeze(2) * out, dim=1)\n",
    "        out = self.fc(context)\n",
    "        return out\n",
    "      \n",
    "attention_model = RNNWithAttentionModel()\n",
    "optimizer = torch.optim.Adam(attention_model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(\"Model Instantiated\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    attention_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    padded_inputs = pad_sequences(inputs)\n",
    "    outputs = attention_model(padded_inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "for input_seq, target in zip(input_data, target_data):\n",
    "    input_test = torch.tensor(input_seq, dtype=torch.long).unsqueeze(0)\n",
    "   \t\n",
    "    #  Set the RNN model to evaluation mode\n",
    "    rnn_model.eval()\n",
    "    # Get the RNN output by passing the appropriate input \n",
    "    rnn_output = rnn_model(input_test)\n",
    "    # Extract the word with the highest prediction score \n",
    "    rnn_prediction = ix_to_word[torch.argmax(rnn_output).item()]\n",
    "\n",
    "    attention_model.eval()\n",
    "    attention_output = attention_model(input_test)\n",
    "    # Extract the word with the highest prediction score\n",
    "    attention_prediction = ix_to_word[torch.argmax(attention_output).item()]\n",
    "\n",
    "    print(f\"\\nInput: {' '.join([ix_to_word[ix] for ix in input_seq])}\")\n",
    "    print(f\"Target: {ix_to_word[target]}\")\n",
    "    print(f\"RNN prediction: {rnn_prediction}\")\n",
    "    print(f\"RNN with Attention prediction: {attention_prediction}\")\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
