{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the decimal returns into percentage returns\n",
    "percent_return = StockPrices['Returns']*100\n",
    "\n",
    "# Drop the missing values\n",
    "returns_plot = percent_return.dropna()\n",
    "\n",
    "# Plot the returns histogram\n",
    "plt.hist(returns_plot, bins=75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import skew from scipy.stats\n",
    "from scipy.stats import skew\n",
    "\n",
    "# Drop the missing values\n",
    "clean_returns = StockPrices['Returns'].dropna()\n",
    "\n",
    "# Calculate the third moment (skewness) of the returns distribution\n",
    "returns_skewness = skew(clean_returns)\n",
    "print(returns_skewness)\n",
    "\n",
    "# Import kurtosis from scipy.stats\n",
    "from scipy.stats import kurtosis\n",
    "\n",
    "# Calculate the excess kurtosis of the returns distribution\n",
    "excess_kurtosis = kurtosis(clean_returns)\n",
    "print(excess_kurtosis)\n",
    "\n",
    "# Derive the true fourth moment of the returns distribution\n",
    "fourth_moment = excess_kurtosis+3\n",
    "print(fourth_moment)\n",
    "\n",
    "# Import shapiro from scipy.stats\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "# Run the Shapiro-Wilk test on the stock returns\n",
    "shapiro_results = shapiro(clean_returns)\n",
    "print(\"Shapiro results:\", shapiro_results)\n",
    "\n",
    "# Extract the p-value from the shapiro_results\n",
    "p_value = shapiro_results[1]\n",
    "print(\"P-value: \", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish defining the portfolio weights as a numpy array\n",
    "portfolio_weights = np.array([0.12, 0.15, 0.08, 0.05, 0.09, 0.10, 0.11, 0.14, 0.16])\n",
    "\n",
    "# Calculate the weighted stock returns\n",
    "WeightedReturns = StockReturns.mul(portfolio_weights, axis=1)\n",
    "\n",
    "# Calculate the portfolio returns\n",
    "StockReturns['Portfolio'] = WeightedReturns.sum(axis=1)\n",
    "\n",
    "# Plot the cumulative portfolio returns over time\n",
    "CumulativeReturns = ((1+StockReturns[\"Portfolio\"]).cumprod()-1)\n",
    "CumulativeReturns.plot()\n",
    "plt.show()\n",
    "\n",
    "# How many stocks are in your portfolio?\n",
    "numstocks = 9\n",
    "\n",
    "# Create an array of equal weights across all assets\n",
    "portfolio_weights_ew = np.repeat(1/numstocks, numstocks)\n",
    "\n",
    "# Calculate the equally-weighted portfolio returns\n",
    "StockReturns['Portfolio_EW'] = StockReturns.iloc[:, 0:numstocks].mul(portfolio_weights_ew, axis=1).sum(axis=1)\n",
    "cumulative_returns_plot(['Portfolio', 'Portfolio_EW'])\n",
    "\n",
    "# Create an array of market capitalizations (in billions)\n",
    "market_capitalizations = np.array([601.51, 469.25, 349.5, 310.48, 299.77, 356.94, 268.88, 331.57, 246.09])\n",
    "\n",
    "# Calculate the market cap weights\n",
    "mcap_weights = market_capitalizations / sum(market_capitalizations)\n",
    "\n",
    "# Calculate the market cap weighted portfolio returns\n",
    "StockReturns['Portfolio_MCap'] = StockReturns.iloc[:, 0:9].mul(mcap_weights, axis=1).sum(axis=1)\n",
    "cumulative_returns_plot(['Portfolio', 'Portfolio_EW', 'Portfolio_MCap'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capital Asset Pricing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate excess portfolio returns\n",
    "FamaFrenchData['Portfolio_Excess'] = FamaFrenchData['Portfolio'] - FamaFrenchData['RF']\n",
    "\n",
    "# Plot returns vs excess returns\n",
    "CumulativeReturns = ((1+FamaFrenchData[['Portfolio','Portfolio_Excess']]).cumprod()-1)\n",
    "CumulativeReturns.plot()\n",
    "plt.show()\n",
    "\n",
    "# Calculate the co-variance matrix between Portfolio_Excess and Market_Excess\n",
    "covariance_matrix = FamaFrenchData[['Portfolio_Excess', 'Market_Excess']].cov()\n",
    "\n",
    "# Extract the co-variance co-efficient\n",
    "covariance_coefficient = covariance_matrix.iloc[0, 1]\n",
    "print(covariance_coefficient)\n",
    "\n",
    "# Calculate the benchmark variance\n",
    "benchmark_variance = FamaFrenchData['Market_Excess'].var()\n",
    "print(benchmark_variance)\n",
    "\n",
    "# Calculating the portfolio market beta\n",
    "portfolio_beta = covariance_coefficient / benchmark_variance\n",
    "print(portfolio_beta)\n",
    "\n",
    "# Import statsmodels.formula.api\n",
    "import statsmodels.formula.api as smf \n",
    "\n",
    "# Define the regression formula\n",
    "CAPM_model = smf.ols(formula='Portfolio_Excess ~ Market_Excess', data=FamaFrenchData)\n",
    "\n",
    "# Print adjusted r-squared of the fitted regression\n",
    "CAPM_fit = CAPM_model.fit()\n",
    "print(CAPM_fit.rsquared_adj)\n",
    "\n",
    "# Extract the beta\n",
    "regression_beta = CAPM_fit.params['Market_Excess']\n",
    "print(regression_beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fama-French 3 factor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statsmodels.formula.api\n",
    "import statsmodels.formula.api as smf \n",
    "\n",
    "# Define the regression formula\n",
    "FamaFrench_model = smf.ols(formula='Portfolio_Excess ~ Market_Excess + SMB + HML', data=FamaFrenchData)\n",
    "\n",
    "# Fit the regression\n",
    "FamaFrench_fit = FamaFrench_model.fit()\n",
    "\n",
    "# Extract the adjusted r-squared\n",
    "regression_adj_rsq = FamaFrench_fit.rsquared_adj\n",
    "print(regression_adj_rsq)\n",
    "\n",
    "# Extract the p-value of the SMB factor\n",
    "smb_pval = FamaFrench_fit.pvalues['SMB']\n",
    "\n",
    "# If the p-value is significant, print significant\n",
    "if smb_pval < 0.05:\n",
    "    significant_msg = 'significant'\n",
    "else:\n",
    "    significant_msg = 'not significant'\n",
    "\n",
    "# Print the SMB coefficient\n",
    "smb_coeff = FamaFrench_fit.params['SMB']\n",
    "print(\"The SMB coefficient is \", smb_coeff, \" and is \", significant_msg)\n",
    "\n",
    "# Calculate your portfolio alpha\n",
    "portfolio_alpha = FamaFrench_fit.params['Intercept']\n",
    "print(portfolio_alpha)\n",
    "\n",
    "# Annualize your portfolio alpha\n",
    "portfolio_alpha_annualized = ((1 + portfolio_alpha)**252)-1\n",
    "print(portfolio_alpha_annualized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statsmodels.formula.api\n",
    "import statsmodels.formula.api as smf \n",
    "\n",
    "# Define the regression formula\n",
    "FamaFrench5_model = smf.ols(formula='Portfolio_Excess ~ Market_Excess + SMB + HML + RMW + CMA ', data=FamaFrenchData)\n",
    "\n",
    "# Fit the regression\n",
    "FamaFrench5_fit = FamaFrench5_model.fit()\n",
    "\n",
    "# Extract the adjusted r-squared\n",
    "regression_adj_rsq = FamaFrench5_fit.rsquared_adj\n",
    "print(regression_adj_rsq)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimating Tail Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historical Draw Down\n",
    "# Calculate the running maximum\n",
    "running_max = np.maximum.accumulate(cum_rets)\n",
    "\n",
    "# Ensure the value never drops below 1\n",
    "running_max[running_max < 1] = 1\n",
    "\n",
    "# Calculate the percentage drawdown\n",
    "drawdown = (cum_rets)/running_max - 1\n",
    "\n",
    "# Plot the results\n",
    "drawdown.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historical Values at Risk\n",
    "\n",
    "# Calculate historical VaR(95)\n",
    "var_95 = np.percentile(StockReturns_perc, 5)\n",
    "print(var_95)\n",
    "\n",
    "# Sort the returns for plotting\n",
    "sorted_rets = StockReturns_perc.sort_values()\n",
    "\n",
    "# Plot the probability of each sorted return quantile\n",
    "plt.hist(sorted_rets, density=True, stacked=True)\n",
    "\n",
    "# Denote the VaR 95 quantile\n",
    "plt.axvline(x=var_95, color='r', linestyle='-', label=\"VaR 95: {0:.2f}%\".format(var_95))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Conditional Values at Risk\n",
    "\n",
    "# Historical CVaR 95\n",
    "cvar_95 = StockReturns_perc[StockReturns_perc<=var_95].mean()\n",
    "print(cvar_95)\n",
    "\n",
    "# Sort the returns for plotting\n",
    "sorted_rets = sorted(StockReturns_perc)\n",
    "\n",
    "# Plot the probability of each return quantile\n",
    "plt.hist(sorted_rets, density=True, stacked=True)\n",
    "\n",
    "# Denote the VaR 95 and CVaR 95 quantiles\n",
    "plt.axvline(x=var_95, color=\"r\", linestyle=\"-\", label='VaR 95: {0:.2f}%'.format(var_95))\n",
    "plt.axvline(x=cvar_95, color='b', linestyle='-', label='CVaR 95: {0:.2f}%'.format(cvar_95))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import norm from scipy.stats\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Estimate the average daily return\n",
    "mu = np.mean(StockReturns)\n",
    "\n",
    "# Estimate the daily volatility\n",
    "vol = np.std(StockReturns)\n",
    "\n",
    "# Set the VaR confidence level\n",
    "confidence_level = 0.05\n",
    "\n",
    "# Calculate Parametric VaR\n",
    "var_95 = norm.ppf(confidence_level, mu, vol)\n",
    "print('Mean: ', str(mu), '\\nVolatility: ', str(vol), '\\nVaR(95): ', str(var_95))\n",
    "\n",
    "# Aggregate forecasted VaR\n",
    "forecasted_values = np.empty([100, 2])\n",
    "\n",
    "# Loop through each forecast period\n",
    "for i in range(100):\n",
    "    # Save the time horizon i\n",
    "    forecasted_values[i, 0] = i\n",
    "    # Save the forecasted VaR 95\n",
    "    forecasted_values[i, 1] = var_95*np.sqrt(i+1)\n",
    "    \n",
    "# Plot the results\n",
    "plot_var_scale()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the simulation parameters\n",
    "mu = np.mean(StockReturns)\n",
    "vol = np.std(StockReturns)\n",
    "T = 252\n",
    "S0 = 10\n",
    "\n",
    "# Add one to the random returns\n",
    "rand_rets = np.random.normal(mu, vol, T) + 1\n",
    "\n",
    "# Forecasted random walk\n",
    "forecasted_values = S0 * (rand_rets.cumprod())\n",
    "\n",
    "# Plot the random walk\n",
    "plt.plot(range(0, T), forecasted_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through 100 simulations\n",
    "for i in range(100):\n",
    "\n",
    "    # Generate the random returns\n",
    "    rand_rets = np.random.normal(mu, vol, T) + 1\n",
    "    \n",
    "    # Create the Monte carlo path\n",
    "    forecasted_values = S0*(rand_rets).cumprod()\n",
    "    \n",
    "    # Plot the Monte Carlo path\n",
    "    plt.plot(range(T), forecasted_values)\n",
    "\n",
    "# Show the simulations\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the returns\n",
    "sim_returns = []\n",
    "\n",
    "# Loop through 100 simulations\n",
    "for i in range(100):\n",
    "\n",
    "    # Generate the Random Walk\n",
    "    rand_rets = np.random.normal(mu, vol, T)\n",
    "    \n",
    "    # Save the results\n",
    "    sim_returns.append(rand_rets)\n",
    "\n",
    "# Calculate the VaR(99)\n",
    "var_99 = np.percentile(sim_returns, 1)\n",
    "print(\"Parametric VaR(99): \", round(100*var_99, 2),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Risk Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select portfolio asset prices for the middle of the crisis, 2008-2009\n",
    "asset_prices = portfolio.loc['2008-01-01':'2009-12-31']\n",
    "\n",
    "# Plot portfolio's asset prices during this time\n",
    "asset_prices.plot().set_ylabel(\"Closing Prices, USD\")\n",
    "plt.show()\n",
    "\n",
    "# Compute the portfolio's daily returns\n",
    "asset_returns = asset_prices.pct_change()\n",
    "portfolio_returns = asset_returns.dot(weights)\n",
    "\n",
    "# Plot portfolio returns\n",
    "portfolio_returns.plot().set_ylabel(\"Daily Return, %\")\n",
    "plt.show()\n",
    "\n",
    "# Generate the covariance matrix from portfolio asset's returns\n",
    "covariance = asset_returns.cov()\n",
    "\n",
    "# Annualize the covariance using 252 trading days per year\n",
    "covariance = covariance * 252\n",
    "\n",
    "# Display the covariance matrix\n",
    "print(covariance)\n",
    "\n",
    "# Compute and display portfolio volatility for 2008 - 2009\n",
    "portfolio_variance = np.transpose(weights) @ covariance @ weights\n",
    "portfolio_volatility = np.sqrt(portfolio_variance)\n",
    "print(portfolio_volatility)\n",
    "\n",
    "# Calculate the 30-day rolling window of portfolio returns\n",
    "returns_windowed = portfolio_returns.rolling(30)\n",
    "\n",
    "# Compute the annualized volatility series\n",
    "volatility_series = returns_windowed.std()*np.sqrt(252)\n",
    "\n",
    "# Plot the portfolio volatility\n",
    "volatility_series.plot().set_ylabel(\"Annualized Volatility, 30-day Window\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert daily returns to quarterly average returns\n",
    "returns_q = returns.resample('Q').mean()\n",
    "\n",
    "# Examine the beginning of the quarterly series\n",
    "print(returns_q.head())\n",
    "\n",
    "# Now convert daily returns to weekly minimum returns\n",
    "returns_w = returns.resample('W').min()\n",
    "\n",
    "# Examine the beginning of the weekly series\n",
    "print(returns_w.head())\n",
    "\n",
    "# Transform the daily portfolio_returns into quarterly average returns\n",
    "portfolio_q_average = portfolio_returns.resample('Q').mean().dropna()\n",
    "\n",
    "# Create a scatterplot between delinquency and quarterly average returns\n",
    "plot_average.scatter(mort_del, portfolio_q_average)\n",
    "\n",
    "# Transform daily portfolio_returns returns into quarterly minimum returns\n",
    "portfolio_q_min = portfolio_returns.resample('Q').min().dropna()\n",
    "\n",
    "# Create a scatterplot between delinquency and quarterly minimum returns\n",
    "plot_min.scatter(mort_del, portfolio_q_min)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a constant to the regression\n",
    "mort_del = sm.add_constant(mort_del)\n",
    "\n",
    "# Create the regression factor model and fit it to the data\n",
    "results = sm.OLS(port_q_mean, mort_del).fit()\n",
    "\n",
    "# Print a summary of the results\n",
    "print(results.summary())\n",
    "\n",
    "# Add a constant to the regression\n",
    "mort_del = sm.add_constant(mort_del)\n",
    "\n",
    "# Create the regression factor model and fit it to the data\n",
    "results = sm.OLS(port_q_min, mort_del).fit()\n",
    "\n",
    "# Print a summary of the results\n",
    "print(results.summary())\n",
    "\n",
    "# Add a constant to the regression\n",
    "mort_del = sm.add_constant(mort_del)\n",
    "\n",
    "# Create the regression factor model and fit it to the data\n",
    "results = sm.OLS(vol_q_mean, mort_del).fit()\n",
    "\n",
    "# Print a summary of the results\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the investment portfolio price data into the price variable.\n",
    "prices = pd.read_csv(\"portfolio.csv\")\n",
    "\n",
    "# Convert the 'Date' column to a datetime index\n",
    "prices['Date'] = pd.to_datetime(prices['Date'], format='%d/%m/%Y')\n",
    "prices.set_index(['Date'], inplace = True)\n",
    "\n",
    "# Import the mean_historical_return method\n",
    "from pypfopt.expected_returns import mean_historical_return\n",
    "\n",
    "# Compute the annualized average historical return\n",
    "mean_returns = mean_historical_return(prices, frequency = 252)\n",
    "\n",
    "# Plot the annualized average historical return\n",
    "plt.plot(mean_returns, linestyle = 'None', marker = 'o')\n",
    "plt.show()\n",
    "\n",
    "# Import the CovarianceShrinkage object\n",
    "from pypfopt.risk_models import CovarianceShrinkage\n",
    "\n",
    "# Create the CovarianceShrinkage instance variable\n",
    "cs = CovarianceShrinkage(prices)\n",
    "\n",
    "# Compute the sample covariance matrix of returns\n",
    "sample_cov = prices.pct_change().cov() * 252\n",
    "\n",
    "# Compute the efficient covariance matrix of returns\n",
    "e_cov = cs.ledoit_wolf()\n",
    "\n",
    "# Display both the sample covariance_matrix and the efficient e_cov estimate\n",
    "print(\"Sample Covariance Matrix\\n\", sample_cov, \"\\n\")\n",
    "print(\"Efficient Covariance Matrix\\n\", e_cov, \"\\n\")\n",
    "\n",
    "# Create a dictionary of time periods (or 'epochs')\n",
    "epochs = { 'before' : {'start': '1-1-2005', 'end': '31-12-2006'},\n",
    "           'during' : {'start': '1-1-2007', 'end': '31-12-2008'},\n",
    "           'after'  : {'start': '1-1-2009', 'end': '31-12-2010'}\n",
    "         }\n",
    "\n",
    "# Compute the efficient covariance for each epoch\n",
    "e_cov = {}\n",
    "for x in epochs.keys():\n",
    "  sub_price = prices.loc[epochs[x]['start']:epochs[x]['end']]\n",
    "  e_cov[x] = CovarianceShrinkage(sub_price).ledoit_wolf()\n",
    "\n",
    "# Display the efficient covariance matrices for all epochs\n",
    "print(\"Efficient Covariance Matrices\\n\", e_cov)\n",
    "\n",
    "# Initialize the Crtical Line Algorithm object\n",
    "efficient_portfolio_during = CLA(returns_during, ecov_during)\n",
    "\n",
    "# Find the minimum volatility portfolio weights and display them\n",
    "print(efficient_portfolio_during.min_volatility())\n",
    "\n",
    "# Compute the efficient frontier\n",
    "(ret, vol, weights) = efficient_portfolio_during.efficient_frontier()\n",
    "\n",
    "# Add the frontier to the plot showing the 'before' and 'after' frontiers\n",
    "plt.scatter(vol, ret, s = 4, c = 'g', marker = '.', label = 'During')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the VaR measure at the 95% confidence level using norm.ppf()\n",
    "VaR_95 = norm.ppf(0.95)\n",
    "\n",
    "# Create the VaR measure at the 99% confidence level using numpy.quantile()\n",
    "draws = norm.rvs(size = 100000)\n",
    "VaR_99 = np.quantile(draws, 0.99)\n",
    "\n",
    "# Compare the 95% and 99% VaR\n",
    "print(\"95% VaR: \", VaR_95, \"; 99% VaR: \", VaR_99)\n",
    "\n",
    "# Plot the normal distribution histogram and 95% VaR measure\n",
    "plt.hist(draws, bins = 100)\n",
    "plt.axvline(x = VaR_95, c='r', label = \"VaR at 95% Confidence Level\")\n",
    "plt.legend(); plt.show()\n",
    "\n",
    "# Compute the mean and standard deviation of the portfolio returns\n",
    "pm = portfolio_losses.mean()\n",
    "ps = portfolio_losses.std()\n",
    "\n",
    "# Compute the 95% VaR using the .ppf()\n",
    "VaR_95 = norm.ppf(0.95, loc = pm, scale = ps)\n",
    "# Compute the expected tail loss and the CVaR in the worst 5% of cases\n",
    "tail_loss = norm.expect(lambda x: x, loc = pm, scale = ps, lb = VaR_95)\n",
    "CVaR_95 = (1 / (1 - 0.95)) * tail_loss\n",
    "\n",
    "# Plot the normal distribution histogram and add lines for the VaR and CVaR\n",
    "plt.hist(norm.rvs(size = 100000, loc = pm, scale = ps), bins = 100)\n",
    "plt.axvline(x = VaR_95, c='r', label = \"VaR, 95% confidence level\")\n",
    "plt.axvline(x = CVaR_95, c='g', label = \"CVaR, worst 5% of outcomes\")\n",
    "plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Student's t-distribution\n",
    "from scipy.stats import t\n",
    "\n",
    "# Create rolling window parameter list\n",
    "mu = losses.rolling(30).mean()\n",
    "sigma = losses.rolling(30).std()\n",
    "rolling_parameters = [(29, mu[i], s) for i,s in enumerate(sigma)]\n",
    "\n",
    "# Compute the 99% VaR array using the rolling window parameters\n",
    "VaR_99 = np.array( [ t.ppf(0.99, *params) \n",
    "                    for params in rolling_parameters ] )\n",
    "\n",
    "# Plot the minimum risk exposure over the 2005-2010 time period\n",
    "plt.plot(losses.index, 0.01 * VaR_99 * 100000)\n",
    "plt.show()\n",
    "\n",
    "# Fit the Student's t distribution to crisis losses\n",
    "p = t.fit(crisis_losses)\n",
    "\n",
    "# Compute the VaR_99 for the fitted distribution\n",
    "VaR_99 = t.ppf(0.99, *p)\n",
    "\n",
    "# Use the fitted parameters and VaR_99 to compute CVaR_99\n",
    "tail_loss = t.expect(lambda y: y, args = (p[0],), loc = p[1], scale = p[2], lb = VaR_99 )\n",
    "CVaR_99 = (1 / (1 - 0.99)) * tail_loss\n",
    "print(CVaR_99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the fitted distribution with a plot\n",
    "x = np.linspace(-0.25,0.25,1000)\n",
    "plt.plot(x,fitted.evaluate(x))\n",
    "plt.show()\n",
    "\n",
    "# Create a random sample of 100,000 observations from the fitted distribution\n",
    "sample = fitted.resample(100000)\n",
    "\n",
    "# Compute and display the 95% VaR from the random sample\n",
    "VaR_95 = np.quantile(sample, 0.95)\n",
    "print(VaR_95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the EfficientCVaR class\n",
    "from pypfopt.efficient_frontier import EfficientCVaR\n",
    "\n",
    "# Create the efficient frontier for CVaR minimization\n",
    "ec = EfficientCVaR(None, returns)\n",
    "\n",
    "# Find the cVaR-minimizing portfolio weights at the default 95% confidence level\n",
    "optimal_weights = ec.min_cvar()\n",
    "\n",
    "# Map the values in optimal_weights to the bank names\n",
    "optimal_weights = { names[i] : optimal_weights[i] for i in optimal_weights}\n",
    "\n",
    "# Display the optimal weights\n",
    "print(optimal_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the efficient portfolio dictionary\n",
    "ec_dict = {}\n",
    "\n",
    "# For each epoch, assign an efficient frontier cvar instance to ec\n",
    "for x in ['before', 'during', 'after']: \n",
    "    ec_dict[x] = EfficientCVaR(None, returns_dict[x])\n",
    "    \n",
    "# Initialize the dictionary of optimal weights\n",
    "opt_wts_dict = {}\n",
    "\n",
    "# Find and display the CVaR-minimizing portfolio weights at the default 95% confidence level\n",
    "for x in ['before', 'during', 'after']:\n",
    "    opt_wts_dict[x] = ec_dict[x].min_cvar()\n",
    "    # map bank names to optimal weights\n",
    "    opt_wts_dict[x] = {names[i] : opt_wts_dict[x][i] for i in opt_wts_dict[x]}\n",
    "\n",
    "# Compare the CVaR-minimizing weights to the minimum volatility weights for the 'before' epoch\n",
    "print(\"CVaR:\\n\", pd.DataFrame.from_dict(opt_wts_dict['before']), \"\\n\")\n",
    "print(\"Min Vol:\\n\", pd.DataFrame.from_dict(min_vol_dict['before']), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Black-Scholes to price options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the volatility as the annualized standard deviation of IBM returns\n",
    "sigma = np.sqrt(252) * IBM_returns.std()\n",
    "\n",
    "# Compute the Black-Scholes option price for this volatility\n",
    "value_s = black_scholes(S = 90, X = 80, T = 0.5, r = 0.02, \n",
    "                        sigma = sigma, option_type = \"call\")\n",
    "\n",
    "# Compute the Black-Scholes option price for twice the volatility\n",
    "value_2s = black_scholes(S = 90, X = 80, T = 0.5, r = 0.02, \n",
    "                sigma = 2*sigma, option_type = \"call\")\n",
    "\n",
    "# Display and compare both values\n",
    "print(\"Option value for sigma: \", value_s, \"\\n\",\n",
    "      \"Option value for 2 * sigma: \", value_2s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first 100 observations of IBM data\n",
    "IBM_spot = IBM[:100]\n",
    "\n",
    "# Initialize the European put option values array\n",
    "option_values = np.zeros(IBM_spot.size)\n",
    "\n",
    "# Iterate through IBM's spot price and compute the option values\n",
    "for i,S in enumerate(IBM_spot.values):\n",
    "    option_values[i] = black_scholes(S = S, X = 140, T = 0.5, r = 0.02, \n",
    "                        sigma = sigma, option_type = \"put\")\n",
    "\n",
    "# Display the option values array\n",
    "option_axis.plot(option_values, color = \"red\", label = \"Put Option\")\n",
    "option_axis.legend(loc = \"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the annualized standard deviation of `IBM` returns\n",
    "sigma = np.sqrt(252) * IBM_returns.std()\n",
    "\n",
    "# Compute the Black-Scholes value at IBM spot price 70\n",
    "value = black_scholes(S = 70, X = 80, T = 0.5, r = 0.02, \n",
    "                      sigma = sigma, option_type = \"put\")\n",
    "# Find the delta of the option at IBM spot price 70\n",
    "delta = bs_delta(S = 70, X = 80, T = 0.5, r = 0.02, \n",
    "                 sigma = sigma, option_type = \"put\")\n",
    "\n",
    "# Find the option value change when the price of IBM falls to 69.5\n",
    "value_change = black_scholes(S = 69.5, X = 80, T = 0.5, r = 0.02, \n",
    "                             sigma = sigma, option_type = \"put\") - value\n",
    "\n",
    "print( (69.5 - 70) + (1/delta) * value_change )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking if the distribution is normal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Normal distribution and skewness test from scipy.stats\n",
    "from scipy.stats import norm, anderson\n",
    "\n",
    "# Fit portfolio losses to the Normal distribution\n",
    "params = norm.fit(losses)\n",
    "\n",
    "# Compute the 95% VaR from the fitted distribution, using parameter estimates\n",
    "VaR_95 = norm.ppf(0.95, *params)\n",
    "print(\"VaR_95, Normal distribution: \", VaR_95)\n",
    "\n",
    "# Test the data for Normality\n",
    "print(\"Anderson-Darling test result: \", anderson(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the skew-normal distribution and skewness test from scipy.stats\n",
    "from scipy.stats import skewnorm, skewtest\n",
    "\n",
    "# Test the data for skewness\n",
    "print(\"Skewtest result: \", skewtest(losses))\n",
    "\n",
    "# Fit the portfolio loss data to the skew-normal distribution\n",
    "params = skewnorm.fit(losses)\n",
    "\n",
    "# Compute the 95% VaR from the fitted distribution, using parameter estimates\n",
    "VaR_95 = skewnorm.ppf(0.95, *params)\n",
    "print(\"VaR_95 from skew-normal: \", VaR_95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create portfolio returns for the two sub-periods using the list of asset returns\n",
    "portfolio_returns = np.array([ x.dot(weights) for x in asset_returns])\n",
    "\n",
    "# Derive portfolio losses from portfolio returns\n",
    "losses = - portfolio_returns\n",
    "\n",
    "# Find the historical simulated VaR estimates\n",
    "VaR_95 = [np.quantile(x, 0.95) for x in losses]\n",
    "\n",
    "# Display the VaR estimates\n",
    "print(\"VaR_95, 2005-2006: \", VaR_95[0], '; VaR_95, 2007-2009: ', VaR_95[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize daily cumulative loss for the assets, across N runs\n",
    "daily_loss = np.zeros((4,N))\n",
    "\n",
    "# Create the Monte Carlo simulations for N runs\n",
    "for n in range(N):\n",
    "    # Compute simulated path of length total_steps for correlated returns\n",
    "    correlated_randomness = e_cov @ norm.rvs(size = (4,total_steps))\n",
    "    # Adjust simulated path by total_steps and mean of portfolio losses\n",
    "    steps = 1/total_steps\n",
    "    minute_losses = mu * steps + correlated_randomness * np.sqrt(steps)\n",
    "    daily_loss[:, n] = minute_losses.sum(axis=1)\n",
    "    \n",
    "# Generate the 95% VaR estimate\n",
    "losses = weights @ daily_loss\n",
    "print(\"Monte Carlo VaR_95 estimate: \", np.quantile(losses, 0.95))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structural Breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot of quarterly minimum portfolio returns\n",
    "plt.plot(port_q_min, label=\"Quarterly minimum return\")\n",
    "\n",
    "# Create a plot of quarterly mean volatility\n",
    "plt.plot(vol_q_mean, label=\"Quarterly mean volatility\")\n",
    "\n",
    "# Create legend and plot\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the statsmodels API to be able to run regressions\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Add a constant to the regression\n",
    "mort_del = sm.add_constant(mort_del)\n",
    "\n",
    "# Regress quarterly minimum portfolio returns against mortgage delinquencies\n",
    "result = sm.OLS(port_q_min, mort_del).fit()\n",
    "\n",
    "# Retrieve the sum-of-squared residuals\n",
    "ssr_total = result.ssr\n",
    "print(\"Sum-of-squared residuals, 2005-2010: \", ssr_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add intercept constants to each sub-period 'before' and 'after'\n",
    "before_with_intercept = sm.add_constant(before['mort_del'])\n",
    "after_with_intercept  = sm.add_constant(after['mort_del'])\n",
    "\n",
    "# Fit OLS regressions to each sub-period\n",
    "r_b = sm.OLS(before['returns'], before_with_intercept).fit()\n",
    "r_a = sm.OLS(after['returns'],  after_with_intercept).fit()\n",
    "\n",
    "# Get sum-of-squared residuals for both regressions\n",
    "ssr_before = r_b.ssr\n",
    "ssr_after = r_a.ssr\n",
    "# Compute and display the Chow test statistic\n",
    "numerator = ((ssr_total - (ssr_before + ssr_after)) / 2)\n",
    "denominator = ((ssr_before + ssr_after) / (24 - 4))\n",
    "print(\"Chow test statistic: \", numerator / denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the time series of returns with and without Citibank\n",
    "ret_with_citi = prices_with_citi.pct_change().dot(weights_with_citi)\n",
    "ret_without_citi = prices_without_citi.pct_change().dot(weights_without_citi)\n",
    "\n",
    "# Find the average 30-day rolling window volatility as the standard deviation\n",
    "vol_with_citi = ret_with_citi.rolling(30).std().dropna().rename(\"With Citi\")\n",
    "vol_without_citi = ret_without_citi.rolling(30).std().dropna().rename(\"Without Citi\")\n",
    "\n",
    "# Combine two volatilities into one Pandas DataFrame\n",
    "vol = pd.concat([vol_with_citi, vol_without_citi], axis=1)\n",
    "\n",
    "# Plot volatilities over time\n",
    "vol.plot().set_ylabel(\"Losses\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the 95% VaR on 2009-2010 losses\n",
    "VaR_95 = np.quantile(estimate_data, 0.95)\n",
    "\n",
    "# Find backtest_data exceeding the 95% VaR\n",
    "extreme_values = backtest_data[backtest_data > VaR_95]\n",
    "\n",
    "# Compare the fraction of extreme values for 2007-2008 to the Var_95 estimate\n",
    "print(\"VaR_95: \", VaR_95, \"; Backtest: \", len(extreme_values) / len(backtest_data) )\n",
    "\n",
    "# Plot the extreme values and look for clustering\n",
    "plt.stem(extreme_values.index, extreme_values)\n",
    "plt.ylabel(\"Extreme values > VaR_95\"); plt.xlabel(\"Date\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extreme Value Theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the data into weekly blocks\n",
    "weekly_maxima = losses.resample(\"W\").max()\n",
    "\n",
    "# Plot the resulting weekly maxima\n",
    "axis_1.plot(weekly_maxima, label = \"Weekly Maxima\")\n",
    "axis_1.legend()\n",
    "plt.figure(\"weekly\")\n",
    "plt.show()\n",
    "\n",
    "# Resample the data into monthly blocks\n",
    "monthly_maxima = losses.resample(\"M\").max()\n",
    "\n",
    "# Plot the resulting monthly maxima\n",
    "axis_2.plot(monthly_maxima, label = \"Monthly Maxima\")\n",
    "axis_2.legend()\n",
    "plt.figure(\"monthly\")\n",
    "plt.show()\n",
    "\n",
    "# Resample the data into quarterly blocks\n",
    "quarterly_maxima = losses.resample('Q').max()\n",
    "\n",
    "# Plot the resulting quarterly maxima\n",
    "axis_3.plot(quarterly_maxima, label = \"Quarterly Maxima\")\n",
    "axis_3.legend()\n",
    "plt.figure(\"quarterly\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the log daily losses of GE over the period 2007-2009\n",
    "losses.plot()\n",
    "\n",
    "# Find all daily losses greater than 10%\n",
    "extreme_losses = losses[losses>0.10]\n",
    "\n",
    "# Scatter plot the extreme losses\n",
    "extreme_losses.plot(style='o')\n",
    "plt.show()\n",
    "\n",
    "# Fit extreme distribution to weekly maximum of losses\n",
    "fitted = genextreme.fit(weekly_max)\n",
    "\n",
    "# Plot extreme distribution with weekly max losses historgram\n",
    "x = np.linspace(min(weekly_max), max(weekly_max), 100)\n",
    "plt.plot(x, genextreme.pdf(x, *fitted))\n",
    "plt.hist(weekly_max, 50, density = True, alpha = 0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GEV risk estimation\n",
    "# Compute the weekly block maxima for GE's stock\n",
    "weekly_maxima = losses.resample(\"W\").max()\n",
    "\n",
    "# Fit the GEV distribution to the maxima\n",
    "p = genextreme.fit(weekly_maxima)\n",
    "\n",
    "# Compute the 99% VaR (needed for the CVaR computation)\n",
    "VaR_99 = genextreme.ppf(0.99, *p)\n",
    "\n",
    "# Compute the 99% CVaR estimate\n",
    "CVaR_99 = (1 / (1 - 0.99)) * genextreme.expect(lambda x: x, \n",
    "           args=(p[0],), loc = p[1], scale = p[2], lb = VaR_99)\n",
    "\n",
    "# Display the covering loss amount\n",
    "print(\"Reserve amount: \", 1000000 * CVaR_99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel Density Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a fitted T distribution over losses\n",
    "params = t.fit(losses)\n",
    "\n",
    "# Generate a Gaussian kernal density estimate over losses\n",
    "kde = gaussian_kde(losses)\n",
    "\n",
    "# Add the PDFs of both estimates to a histogram, and display\n",
    "loss_range = np.linspace(np.min(losses), np.max(losses), 1000)\n",
    "axis.plot(loss_range, t.pdf(loss_range, *params), label = 'T distribution')\n",
    "axis.plot(loss_range, kde.pdf(loss_range), label = 'Gaussian KDE')\n",
    "plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the VaR as a quantile of random samples from the distributions\n",
    "VaR_99_T   = np.quantile(t.rvs(size=1000, *p), 0.99)\n",
    "VaR_99_KDE = np.quantile(kde.resample(size=1000), 0.99)\n",
    "\n",
    "# Find the expected tail losses, with lower bounds given by the VaR measures\n",
    "integral_T   = t.expect(lambda x: x, args = (p[0],), loc = p[1], scale = p[2], lb = VaR_99_T) \n",
    "integral_KDE = kde.expect(lambda x: x, lb = VaR_99_KDE) # kde.expect() only available in Datacamp\n",
    "\n",
    "# Create the 99% CVaR estimates\n",
    "CVaR_99_T   = (1 / (1 - 0.99)) * integral_T\n",
    "CVaR_99_KDE = (1 / (1 - 0.99)) * integral_KDE\n",
    "\n",
    "# Display the results\n",
    "print(\"99% CVaR for T: \", CVaR_99_T, \"; 99% CVaR for KDE: \", CVaR_99_KDE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mockup to get familiar with neiraul network\n",
    "# Create the training values from the square root function\n",
    "y = np.sqrt(x)\n",
    "\n",
    "# Create the neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=1, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Train the network\n",
    "model.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
    "model.fit(x, y, epochs=100)\n",
    "\n",
    "## Plot the resulting approximation and the training values\n",
    "plt.plot(x, y, x, model.predict(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the input and output data\n",
    "training_input = prices.drop('Morgan Stanley', axis=1)\n",
    "training_output = prices['Morgan Stanley']\n",
    "\n",
    "# Create and train the neural network with two hidden layers\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=3, activation='sigmoid')) # First hidden layer with 16 nerons and 3 inputs\n",
    "model.add(Dense(8, activation='relu')) # Second hidden layere with 8 nerons\n",
    "model.add(Dense(1)) # Output of one\n",
    "\n",
    "model.compile(loss='mean_squared_logarithmic_error', optimizer='rmsprop')\n",
    "model.fit(training_input, training_output, epochs=100)\n",
    "\n",
    "# Scatter plot of the resulting model prediction\n",
    "axis.scatter(training_output, model.predict(training_input)); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
